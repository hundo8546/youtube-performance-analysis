{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba81d87-00ab-45a0-8da4-50c8a9e49a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import re\n",
    "import shap\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a46b26-78fa-4326-9760-e86d4f8fe7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = Path(\"../data_pipeline/output/data\") # I pulled all data down locally since I generated them, but source data are stored in S3\n",
    "\n",
    "# Iterate over all `.json` files in target directory.\n",
    "video_data = []\n",
    "for json_file in json_dir.glob(\"*.json\"):\n",
    "    print(json_file.name)\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)  # Load JSON content\n",
    "        video_data = video_data + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3b407-6af1-48fe-b294-68a419d10532",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.DataFrame(video_data)\n",
    "videos_df['publish_time'] = pd.to_datetime(videos_df['publish_time'])\n",
    "videos_df['year'] = videos_df['publish_time'].dt.year\n",
    "videos_df['posted_day'] = videos_df['publish_time'].dt.day_name()\n",
    "videos_df = videos_df[~videos_df['view_count'].isna()]\n",
    "videos_df['view_count'] = videos_df['view_count'].astype(int)\n",
    "videos_df['quantile'] = videos_df.groupby('year')['view_count'].transform(lambda x: x.rank(pct=True))\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb88b2-a283-4954-8dbf-98daa2ca6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(videos_df['view_count'], log=True)\n",
    "plt.title(\"View Count Frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831a24-debd-4ddd-b714-63a81205209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROW = 2\n",
    "N_COL = 5\n",
    "fig, ax = plt.subplots(N_ROW, N_COL, figsize=(10,6))\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for y in sorted(videos_df['year'].unique()):\n",
    "    if y == 2005:\n",
    "        continue\n",
    "    df = videos_df[videos_df['year'] == y]\n",
    "    ax[row_idx, col_idx].hist(df['view_count'], log=True)\n",
    "    ax[row_idx, col_idx].set_title(y)\n",
    "    col_idx += 1\n",
    "    if col_idx >= N_COL:\n",
    "        col_idx = 0\n",
    "        row_idx += 1\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67327d2-a0bb-46d9-942e-1a5e85ecd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df.groupby('year')['view_count'].describe().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8a34d-d004-4385-8552-669b8f67024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_columns = videos_df.columns\n",
    "# DROP Years with less than 1000 videos in data set. This should fall away as dataset grows.\n",
    "year_counts = videos_df['year'].value_counts()\n",
    "keep_years = sorted(year_counts[year_counts >= 1000].index)\n",
    "model_df = videos_df[videos_df['year'].isin(keep_years)].copy()\n",
    "# Create categorical variables for labels that appear often enough\n",
    "vocabulary = pd.read_csv(\"~/Downloads/vocabulary.csv\").set_index(\"Index\")\n",
    "all_labels = [label for sublist in model_df['labels'] for label in sublist]\n",
    "label_counts = pd.Series(all_labels).value_counts()\n",
    "frequent_labels = label_counts[label_counts >= 200].index\n",
    "for label in frequent_labels:\n",
    "    model_df[label] = model_df['labels'].apply(lambda x: 1 if label in x else 0)\n",
    "    label_mapping = vocabulary['Name'].to_dict()\n",
    "    # Rename the columns using the text labels instead of numeric IDs\n",
    "    model_df.rename(columns={label: label_mapping[label] for label in frequent_labels}, inplace=True)\n",
    "label_columns = list(set(model_df.columns) - set(previous_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e93f27-2524-4a5a-a8d0-0ecc4d907120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some features are very strongly correlated\n",
    "plt.imshow(model_df[label_columns].corr())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d31eff-4610-4020-be2c-2e389bc42bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(videos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ba3b-691f-4184-a5e3-7f45dd5c45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VIF to identify highly correlated features and remove to address multicollinearity\n",
    "X_vif = model_df[[l for l in label_columns if l != 'year_categorical'] ]\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "vif_data.sort_values(by='VIF', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2e6d4-6c46-4c2a-aa94-d1c97d318a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on VIF, I am removing the following columns:\n",
    "# Cycling (correlated with Bicycle), Smartphone (correlated with Mobilephone), Model aircraft (correlated wtih Radio-controlled aircraft),\n",
    "# Pet (Correlated with specific types of pets)\n",
    "\n",
    "VIF_DROP_LABELS = ['Cycling', 'Smartphone', 'Model aircraft', 'Pet', 'nan']\n",
    "label_columns = list(set(model_df.columns) - set(previous_columns))\n",
    "label_columns = [re.sub(r'[^a-zA-Z_]', '', str(lc)) for lc in label_columns if lc not in VIF_DROP_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be5a3b-5d0c-4728-9649-249f223a4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3706f2-d4ec-4ba9-a38c-4f9bf7ad9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['year_categorical'] = model_df['year'].apply(str)\n",
    "model_df.columns = model_df.columns.str.replace(r'[^a-zA-Z_]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564fd72-c89e-424a-824d-68dc8191cce1",
   "metadata": {},
   "source": [
    "# SHAP values and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf34eb-f5c8-4bb9-bd70-7919cb1fded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['year_categorical'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf4534-e1fc-4926-942a-9c752ddc79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fef2ed-c7ea-41e4-819c-6e718819da15",
   "metadata": {},
   "source": [
    "# TRAIN PREDICTIVE MODELS TO EVALUATE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717f482-1c3c-4a7f-8298-ee46cb0a366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_day_dummies = pd.get_dummies(model_df[\"posted_day\"], prefix=\"posted_day\")\n",
    "year_dummies = pd.get_dummies(model_df[\"year_categorical\"], prefix=\"year\")\n",
    "df_model_full = pd.concat([model_df[[\"quantile\"] + [lc for lc in label_columns if (lc != 'year_categorical') and (lc != 'nan')and (lc != 'youtubem_id')]], posted_day_dummies, year_dummies], axis=1)\n",
    "# removing numeric characters leads to duplciate values for a few variables. Just collapse them together\n",
    "NUMERIC_SUFFIX_VARS = ['CallofDutyModernWarfare', 'Xbox', 'PlayStation']\n",
    "for v in NUMERIC_SUFFIX_VARS:\n",
    "    col_locs = [i for i, col in enumerate(df_model_full.columns) if col == v]\n",
    "    # If htere's anything present for that column, use that.\n",
    "    df_model_full[v] = df_model_full.iloc[:, col_locs].max(axis=1)\n",
    "    # Keep only first one, drop the rest.\n",
    "    cols_to_drop = col_locs[1:]  # Keep the first one\n",
    "    df_model_full.drop(df_model_full.columns[cols_to_drop], axis=1, inplace=True)\n",
    "\n",
    "output_df = pd.DataFrame()\n",
    "mae_results = dict()\n",
    "for yr in keep_years:\n",
    "    if yr+1 not in keep_years: # can't test the following year.\n",
    "        break\n",
    "    df_model = df_model_full[df_model_full[f\"year_{yr}\"]==1]\n",
    "    # Train an XGBoost model for the year\n",
    "    np.random.seed(1885)\n",
    "    X_train = df_model_full[df_model_full[f\"year_{yr}\"]==1].iloc[:,1:]\n",
    "    y_train = df_model_full[df_model_full[f\"year_{yr}\"]==1][\"quantile\"]\n",
    "    X_test = df_model_full[df_model_full[f\"year_{yr+1}\"]==1].iloc[:,1:]\n",
    "    y_test = df_model_full[df_model_full[f\"year_{yr+1}\"]==1][\"quantile\"]\n",
    "    model = xgb.XGBRegressor(max_depth=10, n_estimators=1000, learning_rate=.02)\n",
    "    model.fit(X_train, y_train)\n",
    "    mae_results[yr] = (mean_absolute_error(y_train, model.predict(X_train)), mean_absolute_error(y_test, model.predict(X_test)), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410ddb1-d2c6-4e51-9a3d-6f257f251103",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_keep_years = [str(y)[2:] for y in mae_results.keys()]\n",
    "fig, ax = plt.subplots(2,1, figsize=(4,8))\n",
    "ax[0].bar(short_keep_years, [v[0] for v in mae_results.values()])\n",
    "ax[0].set_xticks(short_keep_years)\n",
    "ax[0].set_ylim([0,1])\n",
    "ax[0].set_xlabel(\"Year (2000's)\")\n",
    "ax[0].set_ylabel(\"Mean Absolute Error\")\n",
    "ax[0].set_title(\"Training MAE\")\n",
    "ax[1].bar(short_keep_years, [v[1] for v in mae_results.values()])\n",
    "ax[1].set_xticks(short_keep_years)\n",
    "ax[1].set_ylim([0,1])\n",
    "ax[1].set_xlabel(\"Year (2000's)\")\n",
    "ax[1].set_ylabel(\"Mean Absolute Error\")\n",
    "ax[1].set_title(\"Testing MAE\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"/Users/ryansloan/Desktop/mae.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370c51e-ea68-491a-bf74-536815125b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a456e6a-3b14-463b-9e6e-32873ce45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ac056-d7b5-46ff-a56e-9f06a171cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize overall Test MAE\n",
    "sum([mae_results[y][1]*mae_results[y][2] for y in mae_results.keys()])/sum([mae_results[y][2] for y in mae_results.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7dd91-913a-4be1-9112-aa98da5b9a97",
   "metadata": {},
   "source": [
    "# BUILD FULL FEATURE IMPORTANCES FOR VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9054ef-2f4c-4cae-8784-79be7632553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_day_dummies = pd.get_dummies(model_df[\"posted_day\"], prefix=\"posted_day\")\n",
    "year_dummies = pd.get_dummies(model_df[\"year_categorical\"], prefix=\"year\")\n",
    "df_model_full = pd.concat([model_df[[\"quantile\"] + [lc for lc in label_columns if (lc != 'year_categorical') and (lc != 'nan')and (lc != 'youtubem_id')]], posted_day_dummies, year_dummies], axis=1)\n",
    "# removing numeric characters leads to duplciate values for a few variables. Just collapse them together\n",
    "NUMERIC_SUFFIX_VARS = ['CallofDutyModernWarfare', 'Xbox', 'PlayStation']\n",
    "for v in NUMERIC_SUFFIX_VARS:\n",
    "    col_locs = [i for i, col in enumerate(df_model_full.columns) if col == v]\n",
    "    # If htere's anything present for that column, use that.\n",
    "    df_model_full[v] = df_model_full.iloc[:, col_locs].max(axis=1)\n",
    "    # Keep only first one, drop the rest.\n",
    "    cols_to_drop = col_locs[1:]  # Keep the first one\n",
    "    df_model_full.drop(df_model_full.columns[cols_to_drop], axis=1, inplace=True)\n",
    "\n",
    "output_df = pd.DataFrame()\n",
    "for yr in keep_years:\n",
    "    df_model = df_model_full[df_model_full[f\"year_{yr}\"]==1]\n",
    "    # Train an XGBoost model for the year\n",
    "    X = df_model.iloc[:, 1:]\n",
    "    y = df_model[\"quantile\"]\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X, y)\n",
    "    # Evaluate feature importance using SHAP values\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "    \n",
    "    #shap.summary_plot(shap_values, X, max_display=25, alpha=.1)\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=df_model.columns[1:])\n",
    "    filtered_shap_values = shap_df*df_model[df_model.columns[1:]].reset_index(drop=True).replace(0, np.nan)\n",
    "    reshaped_df = filtered_shap_values.melt(var_name='variable', value_name='value')\n",
    "    print(len(reshaped_df))\n",
    "    reshaped_df = reshaped_df.dropna()\n",
    "    print(len(reshaped_df))\n",
    "    reshaped_df['year'] = yr\n",
    "    output_df = pd.concat([output_df, reshaped_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa8df5-0858-4aaf-a484-cb71af25065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b916b1-6a2d-4cc5-bd2a-820106099890",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[~output_df['variable'].str.contains('year')].to_csv(\"../dashboard/shap_values.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
